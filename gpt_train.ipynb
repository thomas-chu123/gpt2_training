{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 10:15:46.009868: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer, DefaultDataCollator, Trainer, TrainingArguments, BertTokenizerFast, BertForQuestionAnswering\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "# import tensorflow as tf\n",
    "# import plotly.express as px\n",
    "# import plotly.io as pio\n",
    "# import pandas as pd\n",
    "# import math\n",
    "# import os, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForQuestionAnswering: ['distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'qa_outputs.weight', 'qa_outputs.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/tom/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-0dcddd7fc76c9211/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['Unnamed: 0.1', 'Unnamed: 0', 'title', 'abstract'],\n    num_rows: 117592\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"./source/train.json\", split='train')\n",
    "data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\tom\\.cache\\huggingface\\datasets\\CShorten___csv\\CShorten--ML-ArXiv-Papers-0dcddd7fc76c9211\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-5fcced655579dd43.arrow and C:\\Users\\tom\\.cache\\huggingface\\datasets\\CShorten___csv\\CShorten--ML-ArXiv-Papers-0dcddd7fc76c9211\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-10bb285727aaa68b.arrow\n"
     ]
    }
   ],
   "source": [
    "data = data.train_test_split(shuffle = True, seed = 200, test_size=0.2)\n",
    "\n",
    "train = data[\"train\"]\n",
    "val = data[\"test\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=10):   0%|          | 0/94073 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecf7f412fdfa4f43927f96df6603194a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\tom\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\tom\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 1349, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"C:\\Users\\tom\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3329, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"C:\\Users\\tom\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3210, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\tom\\AppData\\Local\\Temp\\ipykernel_14192\\3676267813.py\", line 3, in tokenization\nNameError: name 'tokenizer' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokens\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Apply the tokenizer in batch mode and drop all the columns except the tokenization result\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m train_token \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mabstract\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mUnnamed: 0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mUnnamed: 0.1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m val_token \u001B[38;5;241m=\u001B[39m val\u001B[38;5;241m.\u001B[39mmap(tokenization, batched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, remove_columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabstract\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnnamed: 0\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnnamed: 0.1\u001B[39m\u001B[38;5;124m\"\u001B[39m], num_proc\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:563\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    562\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 563\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    564\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    565\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    566\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:528\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    521\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    523\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    524\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    526\u001B[0m }\n\u001B[0;32m    527\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 528\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    529\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    530\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3046\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3038\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpawning \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_proc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m processes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   3039\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[0;32m   3040\u001B[0m     disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[0;32m   3041\u001B[0m     unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3044\u001B[0m     desc\u001B[38;5;241m=\u001B[39m(desc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (num_proc=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_proc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3045\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3046\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m iflatmap_unordered(\n\u001B[0;32m   3047\u001B[0m         pool, Dataset\u001B[38;5;241m.\u001B[39m_map_single, kwargs_iterable\u001B[38;5;241m=\u001B[39mkwargs_per_job\n\u001B[0;32m   3048\u001B[0m     ):\n\u001B[0;32m   3049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3050\u001B[0m             shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\datasets\\utils\\py_utils.py:1373\u001B[0m, in \u001B[0;36miflatmap_unordered\u001B[1;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[0;32m   1371\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[1;32m-> 1373\u001B[0m [async_result\u001B[38;5;241m.\u001B[39mget() \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[1;32m~\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\datasets\\utils\\py_utils.py:1373\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1371\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[1;32m-> 1373\u001B[0m [\u001B[43masync_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[1;32m~\\PycharmProjects\\gpt2_training\\venv\\lib\\site-packages\\multiprocess\\pool.py:771\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n\u001B[0;32m    770\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 771\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# The tokenization function\n",
    "def tokenization(data):\n",
    "    tokens = tokenizer(data[\"abstract\"], padding=\"max_length\", truncation=True, max_length=300)\n",
    "    return tokens\n",
    "\n",
    "# Apply the tokenizer in batch mode and drop all the columns except the tokenization result\n",
    "train_token = train.map(tokenization, batched = True, remove_columns=[\"title\", \"abstract\", \"Unnamed: 0\", \"Unnamed: 0.1\"], num_proc=10)\n",
    "val_token = val.map(tokenization, batched = True, remove_columns=[\"title\", \"abstract\", \"Unnamed: 0\", \"Unnamed: 0.1\"], num_proc=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create labels as a copy of input_ids\n",
    "def create_labels(text):\n",
    "    text[\"labels\"] = text[\"input_ids\"].copy()\n",
    "    return text\n",
    "\n",
    "# Add the labels column using map()\n",
    "lm_train = train_token.map(create_labels, batched=True, num_proc=10)\n",
    "lm_val = val_token.map(create_labels, batched=True, num_proc=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
